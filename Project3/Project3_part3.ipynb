{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello world\n",
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Word2Vec\n",
    "sc = pyspark.SparkContext()\n",
    "from pyspark.sql.functions import concat_ws, array\n",
    "from pyspark.sql import *\n",
    "import numpy\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Projec3\")\\\n",
    "        .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.load(\"/home/cse587/CSE587/Project3/train.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "#punc = string.punctuation\n",
    "def removePunctuation(s):\n",
    "    s = ' '.join([i for i in s.split() if i not in stopwords])\n",
    "    return re.sub(r'[^\\w\\s]',\"\",s)\n",
    "#data = re.sub(r'[^\\w\\s]',\"\",data)\n",
    "\n",
    "udf = UserDefinedFunction(lambda x: removePunctuation(x), StringType())\n",
    "df = df.withColumn('plot', udf(df.plot))\n",
    "#new_df = df.withColumn(\"plot\",)\n",
    "\n",
    "\n",
    "\n",
    "genreMap = spark.read.load(\"/home/cse587/CSE587/Project3/mapping.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "genreMap = genreMap.withColumnRenamed('_c0','id')\n",
    "genreMap = genreMap.withColumnRenamed('0','genre')\n",
    "\n",
    "#this converts a column into a list for use inside encodeGenre() \n",
    "#since you cant perform computations on a dataframe inside a UDF\n",
    "genreList = genreMap.select(\"genre\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "def encodeGenre(s):\n",
    "    #s : the value of col \"genre\" at some row\n",
    "    retVal=\"\"\n",
    "    #s = re.sub(r'[^\\\\'']',\"\",s)\n",
    "    #print(\"s: \",s)\n",
    "    #print(\"type of s: \",type(s))\n",
    "    if(s == None):\n",
    "        return '00000000000000000000'\n",
    "    #convert from string to list\n",
    "    s = s[1:-1].split(\", \")\n",
    "    #print(\"type of s: \",type(s))\n",
    "    #print(s)\n",
    "    \n",
    "    #remove single quotes, regex wasnt working for some reason\n",
    "    temp=[]\n",
    "    for x in s:\n",
    "        #s.remove(x)\n",
    "        x=x[1:-1]\n",
    "#         print(x)\n",
    "#         index = genreMap[\"0\"==x]\n",
    "#         print(index)\n",
    "        temp.append(x)\n",
    "    s=temp\n",
    "    #print(s)\n",
    "    \n",
    "    #create one hot encoding, \n",
    "    for genre in genreList:\n",
    "        if genre in s:\n",
    "            retVal=retVal+\"1\"\n",
    "        else:\n",
    "            retVal=retVal+\"0\"\n",
    "##### apparetnly it aint kosher to perform df operations inside a udf... so this hour of work was for nothing\n",
    "#     t = genreMap.withColumn(\"encoding\",F.when(genreMap.genre.isin(s),1).otherwise(0))\n",
    "#     #t.show()\n",
    "#     retVal = t.select(\"encoding\").rdd.flatMap(lambda x: x).collect()\n",
    "#     retVal = ''.join(map(str,retVal))\n",
    "    return retVal\n",
    "    \n",
    "    \n",
    "\n",
    "udf2 = UserDefinedFunction(lambda x: encodeGenre(x), StringType())\n",
    "df = df.withColumn('genre',udf2(df.genre))\n",
    "\n",
    "#install numpy if you havent already, otherwise one of the ml.featuer imports will throw an error cause it needs numpy\n",
    "#%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = df.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shlykov hardworking taxi driver Lyosha saxophonist develop bizarre lovehate relationship despite prejudices realize arent different all\n"
     ]
    }
   ],
   "source": [
    "print(mylist[0]['plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#part 1 code\n",
    "#taken from https://github.com/apache/spark/blob/master/examples/src/main/python/ml/tf_idf_example.py\n",
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"hashFeatures\")\n",
    "# featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "# #this will do part 2\n",
    "# idf = IDF(inputCol=\"hashFeatures\", outputCol=\"idf_output\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# featurizedData = idfModel.transform(featurizedData)\n",
    "\n",
    "word2Vec = Word2Vec(inputCol='words',outputCol='features')\n",
    "w2vmodel = word2Vec.fit(wordsData)\n",
    "featurizedData = w2vmodel.transform(wordsData)\n",
    "\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=['vectors','idf_output'],outputCol = 'features')\n",
    "# featurizedData= assembler.transform(featurizedData)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "|  monami|0.5761631727218628|\n",
      "|sorcerer|0.5508697032928467|\n",
      "|anaconda|0.5376604795455933|\n",
      "|disarmed|0.5315737128257751|\n",
      "|tunneler|0.5255194902420044|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synonyms = w2vmodel.findSynonyms('jester', 5)\n",
    "synonyms.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.load(\"/home/cse587/CSE587/Project3/test.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "df_test = df_test.withColumn('plot',udf(df_test.plot))\n",
    "\n",
    "tokenizer2 = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "test_cases = tokenizer2.transform(df_test)\n",
    "\n",
    "# hashingTF2 = HashingTF(inputCol=\"words\", outputCol=\"hashFeatures\")\n",
    "# test_cases = hashingTF2.transform(wordsData2)\n",
    "# idfModel_testing = idf.fit(test_cases)\n",
    "# test_cases = idfModel_testing.transform(test_cases)\n",
    "\n",
    "word2Vec2 = Word2Vec(inputCol='words',outputCol='features')\n",
    "w2vmodel2 = word2Vec2.fit(test_cases)\n",
    "test_cases = w2vmodel2.transform(test_cases)\n",
    "\n",
    "# assembler2 = VectorAssembler(inputCols=['hashFeatures','idf_output'],outputCol = 'features')\n",
    "# test_cases= assembler2.transform(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import substring \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "all_label_set = featurizedData.drop('movie_id', 'movie_name', 'plot', 'words')\n",
    "\n",
    "columns = all_label_set.columns\n",
    "columns = columns[::-1]\n",
    "all_label_set = all_label_set[columns]\n",
    "all_label_set = all_label_set.withColumnRenamed('genre','all_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    #substr(start = 0,len)\n",
    "    x = all_label_set.withColumn(\"label\", all_label_set.all_label.substr(i+1,1).cast(IntegerType()))\n",
    "    x = x.drop('all_label')\n",
    "    lr = LogisticRegression(maxIter = 10,regParam=0.0,elasticNetParam=0.0)\n",
    "    lr_model =lr.fit(x)\n",
    "\n",
    "    #for all data\n",
    "    #predict\n",
    "    print(i)\n",
    "    test_cases = lr_model.transform(test_cases)\n",
    "    test_cases = test_cases.withColumn(genreList[i],test_cases['prediction'].cast(IntegerType()))\n",
    "    test_cases = test_cases.drop('prediction','probability','rawPrediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_cases.filter(test_cases['Drama'] != test_cases['Thriller']).show()\n",
    "#test_cases = test_cases.withColumn('test', test_cases.movie_id.substr(1,2))\n",
    "#test_cases.select('movie_id','test').show()\n",
    "#test_cases = test_cases.drop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cases = test_cases.drop('movie_name', 'plot', 'words','features','rawFeatures')\n",
    "test_cases =test_cases.withColumn('all_genre',concat_ws(' ',array([genreList[i] for i in range(20)])))\n",
    "for i in range(20):\n",
    "    test_cases = test_cases.drop(genreList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(20):\\n    a = test_cases.filter(test_cases[genreList[i]] != 0).filter(test_cases[genreList[i]] != 1)\\n    a.select('movie_id',genreList[i]).show(vertical=True)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(20):\n",
    "    a = test_cases.filter(test_cases[genreList[i]] != 0).filter(test_cases[genreList[i]] != 1)\n",
    "    a.select('movie_id',genreList[i]).show(vertical=True)\n",
    "'''\n",
    "#test_cases.filter(test_cases['movie_id'] < 10).show(vertical=True)\n",
    "\n",
    "#test_cases.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cases.write.option(\"delimiter\", ',').csv('./part3_take3', escapeQuotes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
