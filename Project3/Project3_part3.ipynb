{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello world\n",
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Word2Vec, Word2VecModel\n",
    "sc = pyspark.SparkContext()\n",
    "from pyspark.sql.functions import concat_ws, array\n",
    "from pyspark.sql import *\n",
    "import numpy\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Projec3\")\\\n",
    "        .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.load(\"/home/cse587/CSE587/Project3/train.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "#punc = string.punctuation\n",
    "def removePunctuation(s):\n",
    "    s = ' '.join([i for i in s.split() if i not in stopwords])\n",
    "    return re.sub(r'[^\\w\\s]',\"\",s)\n",
    "#data = re.sub(r'[^\\w\\s]',\"\",data)\n",
    "\n",
    "udf = UserDefinedFunction(lambda x: removePunctuation(x), StringType())\n",
    "df = df.withColumn('plot', udf(df.plot))\n",
    "#new_df = df.withColumn(\"plot\",)\n",
    "\n",
    "\n",
    "\n",
    "genreMap = spark.read.load(\"/home/cse587/CSE587/Project3/mapping.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "genreMap = genreMap.withColumnRenamed('_c0','id')\n",
    "genreMap = genreMap.withColumnRenamed('0','genre')\n",
    "\n",
    "#this converts a column into a list for use inside encodeGenre() \n",
    "#since you cant perform computations on a dataframe inside a UDF\n",
    "genreList = genreMap.select(\"genre\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "def encodeGenre(s):\n",
    "    #s : the value of col \"genre\" at some row\n",
    "    retVal=\"\"\n",
    "    #s = re.sub(r'[^\\\\'']',\"\",s)\n",
    "    #print(\"s: \",s)\n",
    "    #print(\"type of s: \",type(s))\n",
    "    if(s == None):\n",
    "        return '00000000000000000000'\n",
    "    #convert from string to list\n",
    "    s = s[1:-1].split(\", \")\n",
    "    #print(\"type of s: \",type(s))\n",
    "    #print(s)\n",
    "    \n",
    "    #remove single quotes, regex wasnt working for some reason\n",
    "    temp=[]\n",
    "    for x in s:\n",
    "        #s.remove(x)\n",
    "        x=x[1:-1]\n",
    "#         print(x)\n",
    "#         index = genreMap[\"0\"==x]\n",
    "#         print(index)\n",
    "        temp.append(x)\n",
    "    s=temp\n",
    "    #print(s)\n",
    "    \n",
    "    #create one hot encoding, \n",
    "    for genre in genreList:\n",
    "        if genre in s:\n",
    "            retVal=retVal+\"1\"\n",
    "        else:\n",
    "            retVal=retVal+\"0\"\n",
    "##### apparetnly it aint kosher to perform df operations inside a udf... so this hour of work was for nothing\n",
    "#     t = genreMap.withColumn(\"encoding\",F.when(genreMap.genre.isin(s),1).otherwise(0))\n",
    "#     #t.show()\n",
    "#     retVal = t.select(\"encoding\").rdd.flatMap(lambda x: x).collect()\n",
    "#     retVal = ''.join(map(str,retVal))\n",
    "    return retVal\n",
    "    \n",
    "speed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with train\n",
      "done with test\n"
     ]
    }
   ],
   "source": [
    "udf2 = UserDefinedFunction(lambda x: encodeGenre(x), StringType())\n",
    "df = df.withColumn('genre',udf2(df.genre))\n",
    "\n",
    "#install numpy if you havent already, otherwise one of the ml.featuer imports will throw an error cause it needs numpy\n",
    "#%pip install numpy\n",
    "\n",
    "#part 1 code\n",
    "#taken from https://github.com/apache/spark/blob/master/examples/src/main/python/ml/tf_idf_example.py\n",
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"hashFeatures\")\n",
    "# featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "# #this will do part 2\n",
    "# idf = IDF(inputCol=\"hashFeatures\", outputCol=\"idf_output\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# featurizedData = idfModel.transform(featurizedData)\n",
    "if(speed == False):\n",
    "    word2Vec = Word2Vec(inputCol='words',outputCol='features')\n",
    "    w2vmodel = word2Vec.fit(wordsData)\n",
    "    w2vmodel.save('./models/part3w2vtrain')\n",
    "else:\n",
    "    w2vmodel = Word2VecModel.load('./models/part3w2vtrain')\n",
    "    \n",
    "featurizedData = w2vmodel.transform(wordsData)\n",
    "\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=['vectors','idf_output'],outputCol = 'features')\n",
    "# featurizedData= assembler.transform(featurizedData)\n",
    "print('done with train')\n",
    "\n",
    "df_test = spark.read.load(\"/home/cse587/CSE587/Project3/test.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "df_test = df_test.withColumn('plot',udf(df_test.plot))\n",
    "\n",
    "tokenizer2 = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "test_cases = tokenizer2.transform(df_test)\n",
    "\n",
    "# hashingTF2 = HashingTF(inputCol=\"words\", outputCol=\"hashFeatures\")\n",
    "# test_cases = hashingTF2.transform(wordsData2)\n",
    "# idfModel_testing = idf.fit(test_cases)\n",
    "# test_cases = idfModel_testing.transform(test_cases)\n",
    "\n",
    "if(speed == False):\n",
    "    word2Vec2 = Word2Vec(inputCol='words',outputCol='features')\n",
    "    w2vmodel2 = word2Vec2.fit(test_cases)\n",
    "    w2vmodel2.save('./models/part3w2vtest')\n",
    "else:\n",
    "    w2vmodel2 = Word2VecModel.load('./models/part3w2vtest')\n",
    "\n",
    "test_cases = w2vmodel2.transform(test_cases)\n",
    "print('done with test')\n",
    "\n",
    "# assembler2 = VectorAssembler(inputCols=['hashFeatures','idf_output'],outputCol = 'features')\n",
    "# test_cases= assembler2.transform(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.sql.functions import substring \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "all_label_set = featurizedData.drop('movie_id', 'movie_name', 'plot', 'words')\n",
    "\n",
    "columns = all_label_set.columns\n",
    "columns = columns[::-1]\n",
    "all_label_set = all_label_set[columns]\n",
    "all_label_set = all_label_set.withColumnRenamed('genre','all_label')\n",
    "\n",
    "for i in range(20):\n",
    "    lr_model = None\n",
    "    if(speed == False):\n",
    "        x = all_label_set.withColumn(\"label\", all_label_set.all_label.substr(i+1,1).cast(IntegerType()))\n",
    "        x = x.drop('all_label')\n",
    "        lr = LogisticRegression(maxIter = 20,regParam=0.0,elasticNetParam=0.0)\n",
    "        lr_model =lr.fit(x)\n",
    "        lr_model.save('./models/part3lr_'+str(i))\n",
    "    else:\n",
    "        lr_model = LogisticRegressionModel.load('./models/part3lr_'+str(i))\n",
    "\n",
    "    #for all data\n",
    "    #predict\n",
    "    print(i)\n",
    "    test_cases = lr_model.transform(test_cases)\n",
    "    test_cases = test_cases.withColumn(genreList[i],test_cases['prediction'].cast(IntegerType()))\n",
    "    test_cases = test_cases.drop('prediction','probability','rawPrediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " movie_id  | 1335380              \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 1-------------------------\n",
      " movie_id  | 29062594             \n",
      " all_genre | 0 1 0 0 0 0 0 0 0... \n",
      "-RECORD 2-------------------------\n",
      " movie_id  | 9252321              \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 3-------------------------\n",
      " movie_id  | 13455076             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 4-------------------------\n",
      " movie_id  | 24165951             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 5-------------------------\n",
      " movie_id  | 1925869              \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 6-------------------------\n",
      " movie_id  | 10799612             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 7-------------------------\n",
      " movie_id  | 28238240             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 8-------------------------\n",
      " movie_id  | 17124781             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 9-------------------------\n",
      " movie_id  | 28207941             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 10------------------------\n",
      " movie_id  | 19174305             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 11------------------------\n",
      " movie_id  | 18392317             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 12------------------------\n",
      " movie_id  | 34420857             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 13------------------------\n",
      " movie_id  | 4039635              \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 14------------------------\n",
      " movie_id  | 8034072              \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 15------------------------\n",
      " movie_id  | 4016437              \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 16------------------------\n",
      " movie_id  | 1520023              \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 17------------------------\n",
      " movie_id  | 24589422             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 18------------------------\n",
      " movie_id  | 35068740             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "-RECORD 19------------------------\n",
      " movie_id  | 21132951             \n",
      " all_genre | 1 0 0 0 0 0 0 0 0... \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test_cases.filter(test_cases['Drama'] != test_cases['Thriller']).show()\n",
    "#test_cases = test_cases.withColumn('test', test_cases.movie_id.substr(1,2))\n",
    "#test_cases.select('movie_id','test').show()\n",
    "#test_cases = test_cases.drop('test')\n",
    "\n",
    "test_cases = test_cases.drop('movie_name', 'plot', 'words','features','rawFeatures')\n",
    "test_cases =test_cases.withColumn('all_genre',concat_ws(' ',array([genreList[i] for i in range(20)])))\n",
    "for i in range(20):\n",
    "    test_cases = test_cases.drop(genreList[i])\n",
    "\n",
    "'''\n",
    "for i in range(20):\n",
    "    a = test_cases.filter(test_cases[genreList[i]] != 0).filter(test_cases[genreList[i]] != 1)\n",
    "    a.select('movie_id',genreList[i]).show(vertical=True)\n",
    "'''\n",
    "#test_cases.filter(test_cases['movie_id'] < 10).show(vertical=True)\n",
    "\n",
    "test_cases.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cases.write.option(\"delimiter\", ',').csv('./part3_from_video', escapeQuotes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
