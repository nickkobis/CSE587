{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-44d3ba00f964>:10 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9ec4c8de437d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserDefinedFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#install numpy if you havent already, otherwise one of the ml.featuer imports will throw an error cause it needs numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 314\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-44d3ba00f964>:10 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "sc = pyspark.SparkContext()\n",
    "from pyspark.sql import *\n",
    "#install numpy if you havent already, otherwise one of the ml.featuer imports will throw an error cause it needs numpy\n",
    "#%pip install numpy\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "import numpy\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "spark = SparkSession.builder.appName(\"Projec3\")\\\n",
    "        .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.load(\"/home/cse587/CSE587/Project3/train.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "\n",
    "\n",
    "#punc = string.punctuation\n",
    "def removePunctuation(s):\n",
    "    print(\"type of s: \",s)\n",
    "    return re.sub(r'[^\\w\\s]',\"\",s)\n",
    "#data = re.sub(r'[^\\w\\s]',\"\",data)\n",
    "\n",
    "udf = UserDefinedFunction(lambda x: removePunctuation(x), StringType())\n",
    "df = df.select(*[udf(column).alias(\"plot\") if column == \"plot\" else column for column in df.columns])\n",
    "#new_df = df.withColumn(\"plot\",)\n",
    "\n",
    "\n",
    "genreMap = spark.read.load(\"/home/cse587/CSE587/Project3/mapping.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "genreMap = genreMap.withColumnRenamed('_c0','id')\n",
    "genreMap = genreMap.withColumnRenamed('0','genre')\n",
    "\n",
    "#this converts a column into a list for use inside encodeGenre() \n",
    "#since you cant perform computations on a dataframe inside a UDF\n",
    "genreList = genreMap.select(\"genre\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "def encodeGenre(s):\n",
    "    #s : the value of col \"genre\" at some row\n",
    "    retVal=\"\"\n",
    "    #s = re.sub(r'[^\\\\'']',\"\",s)\n",
    "    #print(\"s: \",s)\n",
    "    #print(\"type of s: \",type(s))\n",
    "    \n",
    "    #convert from string to list\n",
    "    s = s[1:-1].split(\", \")\n",
    "    #print(\"type of s: \",type(s))\n",
    "    #print(s)\n",
    "    \n",
    "    #remove single quotes, regex wasnt working for some reason\n",
    "    temp=[]\n",
    "    for x in s:\n",
    "        #s.remove(x)\n",
    "        x=x[1:-1]\n",
    "#         print(x)\n",
    "#         index = genreMap[\"0\"==x]\n",
    "#         print(index)\n",
    "        temp.append(x)\n",
    "    s=temp\n",
    "    #print(s)\n",
    "    \n",
    "    #create one hot encoding, \n",
    "    for genre in genreList:\n",
    "        if genre in s:\n",
    "            retVal=retVal+\"1\"\n",
    "        else:\n",
    "            retVal=retVal+\"0\"\n",
    "##### apparetnly it aint kosher to perform df operations inside a udf... so this hour of work was for nothing\n",
    "#     t = genreMap.withColumn(\"encoding\",F.when(genreMap.genre.isin(s),1).otherwise(0))\n",
    "#     #t.show()\n",
    "#     retVal = t.select(\"encoding\").rdd.flatMap(lambda x: x).collect()\n",
    "#     retVal = ''.join(map(str,retVal))\n",
    "    return retVal\n",
    "    \n",
    "udf2 = UserDefinedFunction(lambda x: encodeGenre(x), StringType())\n",
    "df = df.select(*[udf2(column).alias(\"genre\") if column == \"genre\" else column for column in df.columns])\n",
    "\n",
    "#taken from https://github.com/apache/spark/blob/master/examples/src/main/python/ml/tf_idf_example.py\n",
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "#this will do part 2\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featurizedData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-585a76a0cd17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mLogisticRegressionWithSGD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'featurizedData' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import *\n",
    "\n",
    "LogisticRegressionWithSGD.train(featurizedData,iterations = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
