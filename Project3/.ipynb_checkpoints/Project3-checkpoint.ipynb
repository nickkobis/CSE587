{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hello world\n",
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "sc = pyspark.SparkContext()\n",
    "from pyspark.sql.functions import concat_ws, array\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Projec3\")\\\n",
    "        .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.load(\"/home/cse587/CSE587/Project3/train.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "\n",
    "\n",
    "#punc = string.punctuation\n",
    "def removePunctuation(s):\n",
    "    print(\"type of s: \",s)\n",
    "    return re.sub(r'[^\\w\\s]',\"\",s)\n",
    "#data = re.sub(r'[^\\w\\s]',\"\",data)\n",
    "\n",
    "udf = UserDefinedFunction(lambda x: removePunctuation(x), StringType())\n",
    "df = df.withColumn('plot', udf(df.plot))\n",
    "#new_df = df.withColumn(\"plot\",)\n",
    "\n",
    "genreMap = spark.read.load(\"/home/cse587/CSE587/Project3/mapping.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "genreMap = genreMap.withColumnRenamed('_c0','id')\n",
    "genreMap = genreMap.withColumnRenamed('0','genre')\n",
    "\n",
    "#this converts a column into a list for use inside encodeGenre() \n",
    "#since you cant perform computations on a dataframe inside a UDF\n",
    "genreList = genreMap.select(\"genre\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "def encodeGenre(s):\n",
    "    #s : the value of col \"genre\" at some row\n",
    "    retVal=\"\"\n",
    "    #s = re.sub(r'[^\\\\'']',\"\",s)\n",
    "    #print(\"s: \",s)\n",
    "    #print(\"type of s: \",type(s))\n",
    "    if(s == None):\n",
    "        return '00000000000000000000'\n",
    "    #convert from string to list\n",
    "    s = s[1:-1].split(\", \")\n",
    "    #print(\"type of s: \",type(s))\n",
    "    #print(s)\n",
    "    \n",
    "    #remove single quotes, regex wasnt working for some reason\n",
    "    temp=[]\n",
    "    for x in s:\n",
    "        #s.remove(x)\n",
    "        x=x[1:-1]\n",
    "#         print(x)\n",
    "#         index = genreMap[\"0\"==x]\n",
    "#         print(index)\n",
    "        temp.append(x)\n",
    "    s=temp\n",
    "    #print(s)\n",
    "    \n",
    "    #create one hot encoding, \n",
    "    for genre in genreList:\n",
    "        if genre in s:\n",
    "            retVal=retVal+\"1\"\n",
    "        else:\n",
    "            retVal=retVal+\"0\"\n",
    "##### apparetnly it aint kosher to perform df operations inside a udf... so this hour of work was for nothing\n",
    "#     t = genreMap.withColumn(\"encoding\",F.when(genreMap.genre.isin(s),1).otherwise(0))\n",
    "#     #t.show()\n",
    "#     retVal = t.select(\"encoding\").rdd.flatMap(lambda x: x).collect()\n",
    "#     retVal = ''.join(map(str,retVal))\n",
    "    return retVal\n",
    "    \n",
    "    \n",
    "\n",
    "udf2 = UserDefinedFunction(lambda x: encodeGenre(x), StringType())\n",
    "df = df.withColumn('genre',udf2(df.genre))\n",
    "\n",
    "#install numpy if you havent already, otherwise one of the ml.featuer imports will throw an error cause it needs numpy\n",
    "#%pip install numpy\n",
    "import numpy\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "#taken from https://github.com/apache/spark/blob/master/examples/src/main/python/ml/tf_idf_example.py\n",
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "#this will do part 2\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "\n",
    "df_test = spark.read.load(\"/home/cse587/CSE587/Project3/test.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "df_test = df_test.withColumn('plot',udf(df_test.plot))\n",
    "\n",
    "tokenizer2 = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "wordsData2 = tokenizer2.transform(df_test)\n",
    "hashingTF2 = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "test_cases = hashingTF2.transform(wordsData2)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import substring \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "all_label_set = featurizedData.drop('movie_id', 'movie_name', 'plot', 'words')\n",
    "\n",
    "columns = all_label_set.columns\n",
    "columns = columns[::-1]\n",
    "all_label_set = all_label_set[columns]\n",
    "all_label_set = all_label_set.withColumnRenamed('rawFeatures','features')\n",
    "all_label_set = all_label_set.withColumnRenamed('genre','all_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    #substr(start = 0,len)\n",
    "    x = all_label_set.withColumn(\"label\", all_label_set.all_label.substr(i+1,1).cast(IntegerType()))\n",
    "    x = x.drop('all_label')\n",
    "    lr = LogisticRegression(maxIter = 20,regParam=0.0,elasticNetParam=0.0)\n",
    "    lr_model =lr.fit(x)\n",
    "\n",
    "    #for all data\n",
    "    #predict\n",
    "    print(i)\n",
    "    test_cases = lr_model.transform(test_cases)\n",
    "    test_cases = test_cases.withColumn(genreList[i],test_cases['prediction'].cast(IntegerType()))\n",
    "    test_cases = test_cases.drop('prediction','probability','rawPrediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-03b26b674477>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-03b26b674477>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    test_cases = test_cases.withColumn('test', test_cases.movie_id.UserDefinedFunction(lamda x: len(x)))\u001b[0m\n\u001b[0m                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#test_cases.filter(test_cases['Drama'] != test_cases['Thriller']).show()\n",
    "#test_cases = test_cases.withColumn('test', test_cases.movie_id.substr(1,2))\n",
    "#test_cases.select('movie_id','test').show()\n",
    "#test_cases = test_cases.drop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cases = test_cases.drop('movie_name', 'plot', 'words','features')\n",
    "test_cases =test_cases.withColumn('all_genre',concat_ws(' ',array([genreList[i] for i in range(20)])))\n",
    "for i in range(20):\n",
    "    test_cases = test_cases.drop(genreList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " movie_id  | 1335380              \n",
      " all_genre | 1 1 0 0 0 0 0 0 0... \n",
      "-RECORD 1-------------------------\n",
      " movie_id  | 29062594             \n",
      " all_genre | 1 1 0 0 0 0 0 0 0... \n",
      "-RECORD 2-------------------------\n",
      " movie_id  | 9252321              \n",
      " all_genre | 1 1 0 0 0 0 0 0 0... \n",
      "-RECORD 3-------------------------\n",
      " movie_id  | 13455076             \n",
      " all_genre | 0 0 1 0 0 0 0 0 0... \n",
      "-RECORD 4-------------------------\n",
      " movie_id  | 24165951             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 5-------------------------\n",
      " movie_id  | 1925869              \n",
      " all_genre | 1 1 0 0 0 0 0 0 0... \n",
      "-RECORD 6-------------------------\n",
      " movie_id  | 10799612             \n",
      " all_genre | 1 1 1 0 0 0 0 0 0... \n",
      "-RECORD 7-------------------------\n",
      " movie_id  | 28238240             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 8-------------------------\n",
      " movie_id  | 17124781             \n",
      " all_genre | 0 0 0 0 0 1 0 0 0... \n",
      "-RECORD 9-------------------------\n",
      " movie_id  | 28207941             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 10------------------------\n",
      " movie_id  | 19174305             \n",
      " all_genre | 0 0 0 1 0 0 0 0 0... \n",
      "-RECORD 11------------------------\n",
      " movie_id  | 18392317             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 12------------------------\n",
      " movie_id  | 34420857             \n",
      " all_genre | 1 1 0 0 0 0 1 0 0... \n",
      "-RECORD 13------------------------\n",
      " movie_id  | 4039635              \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "-RECORD 14------------------------\n",
      " movie_id  | 8034072              \n",
      " all_genre | 1 1 0 1 0 0 1 0 0... \n",
      "-RECORD 15------------------------\n",
      " movie_id  | 4016437              \n",
      " all_genre | 1 1 0 0 0 0 0 0 0... \n",
      "-RECORD 16------------------------\n",
      " movie_id  | 1520023              \n",
      " all_genre | 0 0 0 0 0 1 1 0 1... \n",
      "-RECORD 17------------------------\n",
      " movie_id  | 24589422             \n",
      " all_genre | 1 1 0 1 0 0 0 0 0... \n",
      "-RECORD 18------------------------\n",
      " movie_id  | 35068740             \n",
      " all_genre | 0 0 0 1 0 0 0 0 0... \n",
      "-RECORD 19------------------------\n",
      " movie_id  | 21132951             \n",
      " all_genre | 0 0 0 0 0 0 0 0 0... \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i in range(20):\n",
    "    a = test_cases.filter(test_cases[genreList[i]] != 0).filter(test_cases[genreList[i]] != 1)\n",
    "    a.select('movie_id',genreList[i]).show(vertical=True)\n",
    "'''\n",
    "#test_cases.filter(test_cases['movie_id'] < 10).show(vertical=True)\n",
    "#test_cases.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_cases.write.option(\"delimiter\", ',').csv('./part1_fixed.csv', escapeQuotes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
