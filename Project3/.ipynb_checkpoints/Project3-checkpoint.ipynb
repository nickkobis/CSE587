{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello world\n",
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "import string\n",
    "import re\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Projec3\")\\\n",
    "        .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/home/cse587/CSE587/Project3/train.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- plot: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+-----+\n",
      "|movie_id|movie_name|                plot|genre|\n",
      "+--------+----------+--------------------+-----+\n",
      "|20580938| Talentime|A music teacher, ...| null|\n",
      "|33904402|  Deadroom|Four Conversation...| null|\n",
      "+--------+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['genre'].isNull()).show()\n",
    " #formated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#punc = string.punctuation\n",
    "def removePunctuation(s):\n",
    "    print(\"type of s: \",s)\n",
    "    return re.sub(r'[^\\w\\s]',\"\",s)\n",
    "#data = re.sub(r'[^\\w\\s]',\"\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = UserDefinedFunction(lambda x: removePunctuation(x), StringType())\n",
    "df = df.withColumn('plot', udf(df.plot))\n",
    "#new_df = df.withColumn(\"plot\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genreMap = spark.read.load(\"/home/cse587/CSE587/Project3/mapping.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "genreMap = genreMap.withColumnRenamed('_c0','id')\n",
    "genreMap = genreMap.withColumnRenamed('0','genre')\n",
    "\n",
    "#this converts a column into a list for use inside encodeGenre() \n",
    "#since you cant perform computations on a dataframe inside a UDF\n",
    "genreList = genreMap.select(\"genre\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encodeGenre(s):\n",
    "    #s : the value of col \"genre\" at some row\n",
    "    retVal=\"\"\n",
    "    #s = re.sub(r'[^\\\\'']',\"\",s)\n",
    "    #print(\"s: \",s)\n",
    "    #print(\"type of s: \",type(s))\n",
    "    if(s == None):\n",
    "        return '00000000000000000000'\n",
    "    #convert from string to list\n",
    "    s = s[1:-1].split(\", \")\n",
    "    #print(\"type of s: \",type(s))\n",
    "    #print(s)\n",
    "    \n",
    "    #remove single quotes, regex wasnt working for some reason\n",
    "    temp=[]\n",
    "    for x in s:\n",
    "        #s.remove(x)\n",
    "        x=x[1:-1]\n",
    "#         print(x)\n",
    "#         index = genreMap[\"0\"==x]\n",
    "#         print(index)\n",
    "        temp.append(x)\n",
    "    s=temp\n",
    "    #print(s)\n",
    "    \n",
    "    #create one hot encoding, \n",
    "    for genre in genreList:\n",
    "        if genre in s:\n",
    "            retVal=retVal+\"1\"\n",
    "        else:\n",
    "            retVal=retVal+\"0\"\n",
    "##### apparetnly it aint kosher to perform df operations inside a udf... so this hour of work was for nothing\n",
    "#     t = genreMap.withColumn(\"encoding\",F.when(genreMap.genre.isin(s),1).otherwise(0))\n",
    "#     #t.show()\n",
    "#     retVal = t.select(\"encoding\").rdd.flatMap(lambda x: x).collect()\n",
    "#     retVal = ''.join(map(str,retVal))\n",
    "    return retVal\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf2 = UserDefinedFunction(lambda x: encodeGenre(x), StringType())\n",
    "df = df.withColumn('genre',udf2(df.genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install numpy if you havent already, otherwise one of the ml.featuer imports will throw an error cause it needs numpy\n",
    "#%pip install numpy\n",
    "import numpy\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://github.com/apache/spark/blob/master/examples/src/main/python/ml/tf_idf_example.py\n",
    "tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "#this will do part 2\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.load(\"/home/cse587/CSE587/Project3/test.csv\",sep=\",\",format=\"csv\",inferSchema = True,header = True,escape ='\"')\n",
    "df_test = df_test.withColumn('plot',udf(df_test.plot))\n",
    "\n",
    "tokenizer2 = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "wordsData2 = tokenizer2.transform(df_test)\n",
    "hashingTF2 = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "test_cases = hashingTF2.transform(wordsData2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import substring \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "all_label_set = featurizedData.drop('movie_id', 'movie_name', 'plot', 'words')\n",
    "\n",
    "columns = all_label_set.columns\n",
    "columns = columns[::-1]\n",
    "all_label_set = all_label_set[columns]\n",
    "all_label_set = all_label_set.withColumnRenamed('rawFeatures','features')\n",
    "all_label_set = all_label_set.withColumnRenamed('genre','all_label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|movie_id|          movie_name|                plot|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| 1335380|              Exodus|The film is based...|[the, film, is, b...|(262144,[1728,261...|[-40.330896628245...|[3.05150540923104...|       1.0|\n",
      "|29062594|A la salida nos v...|A group of teenag...|[a, group, of, te...|(262144,[6068,963...|[-0.4341972775223...|[0.39312450711486...|       1.0|\n",
      "| 9252321|   Come Back, Africa|This story of a Z...|[this, story, of,...|(262144,[1598,208...|[-4.7906449291459...|[0.00823865881669...|       1.0|\n",
      "|13455076|       A Merry Mixup|The Stooges play ...|[the, stooges, pl...|(262144,[3294,618...|[7.24215325496628...|[0.99928474309190...|       0.0|\n",
      "|24165951|        Getting Even|A soldieroffortun...|[a, soldieroffort...|(262144,[4098,644...|[3.23311490827612...|[0.96206160796277...|       0.0|\n",
      "| 1925869|  River of No Return|Set in the Northw...|[set, in, the, no...|(262144,[535,3294...|[-7.6381758306497...|[4.81474435770601...|       1.0|\n",
      "|10799612|          Amici miei|Like in many othe...|[like, in, many, ...|(262144,[5053,538...|[-21.341516654158...|[5.38886223893831...|       1.0|\n",
      "|28238240|Mickey's Big Game...|Mickey and the Sc...|[mickey, and, the...|(262144,[13142,23...|[1.44008249980959...|[0.80846742668150...|       0.0|\n",
      "|17124781|The Good, the Bad...|In the desert wil...|[in, the, desert,...|(262144,[1652,523...|[9.26482274247007...|[0.99990531144642...|       0.0|\n",
      "|28207941|    The Dancing Fool|Bimbo and Koko ar...|[bimbo, and, koko...|(262144,[9639,972...|[9.36494089873049...|[0.99991433160362...|       0.0|\n",
      "|19174305|              Tahaan|Tahaan  lives wit...|[tahaan, , lives,...|(262144,[2710,392...|[5.47336385831442...|[0.99582045196992...|       0.0|\n",
      "|18392317|     Mysterious Mose|Betty is startled...|[betty, is, start...|(262144,[5213,606...|[12.4560602882007...|[0.99999610596312...|       0.0|\n",
      "|34420857|Kelviyum Naane Pa...|Nirmal Karthik  t...|[nirmal, karthik,...|(262144,[7554,963...|[1.57411351292909...|[0.82836923160983...|       0.0|\n",
      "| 4039635|   First on the Moon|A group of journa...|[a, group, of, jo...|(262144,[571,1640...|[19.2198946330631...|[0.99999999550317...|       0.0|\n",
      "| 8034072|  Journey of a Woman|Vaibhavari Sahay ...|[vaibhavari, saha...|(262144,[991,4200...|[-17.899341686173...|[1.68428146221415...|       1.0|\n",
      "| 4016437|     Sophie's Choice|In 1947 the movie...|[in, 1947, the, m...|(262144,[5595,783...|[-20.674519490018...|[1.04995399180198...|       1.0|\n",
      "| 1520023|  Ninja Resurrection|Ninja Resurrectio...|[ninja, resurrect...|(262144,[14,535,5...|[24.6758480529054...|[0.99999999998079...|       0.0|\n",
      "|24589422|      Maria’s Lovers|In the spring of ...|[in, the, spring,...|(262144,[1998,249...|[-18.070850259546...|[1.41882699477097...|       1.0|\n",
      "|35068740|           Chinnavar|Muthu Prabhu  and...|[muthu, prabhu, ,...|(262144,[2710,484...|[19.7319166657332...|[0.99999999730513...|       0.0|\n",
      "|21132951|              Aparan|Vishwanathan  an ...|[vishwanathan, , ...|(262144,[1841,392...|[7.85322766974915...|[0.99961165486491...|       0.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column rawPrediction already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1343.transform.\n: java.lang.IllegalArgumentException: requirement failed: Column rawPrediction already exists.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.appendColumn(SchemaUtils.scala:105)\n\tat org.apache.spark.ml.util.SchemaUtils$.appendColumn(SchemaUtils.scala:95)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:43)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:77)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:930)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:266)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.validateAndTransformSchema(LogisticRegression.scala:930)\n\tat org.apache.spark.ml.PredictionModel.transformSchema(Predictor.scala:192)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.transform(ProbabilisticClassifier.scala:104)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2ad47dca4db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#for all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtest_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_cases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rawPrediciton'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column rawPrediction already exists.'"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    #substr(start = 0,len)\n",
    "    x = all_label_set.withColumn(\"label\", all_label_set.all_label.substr(i,1).cast(IntegerType()))\n",
    "    x = x.drop('all_label')\n",
    "    lr = LogisticRegression(maxIter = 10,regParam=0.0,elasticNetParam=0.0)\n",
    "    lr_model =lr.fit(x)\n",
    "\n",
    "    #for all data\n",
    "    #predict\n",
    "    print(i)\n",
    "    test_cases = lr_model.transform(test_cases)\n",
    "    test_cases = test_cases.drop('probability','rawPrediction')\n",
    "    test_cases = test_cases.withColumnRenamed('prediction',genreList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
